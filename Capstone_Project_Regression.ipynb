{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TabassumPatel4458/Regression/blob/main/Capstone_Project_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Bike Sharing Demand Prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Team (Data Pirates)\n",
        "##### *Team Member 1 -Tabassum Patel (Team Leader)*\n",
        "##### *Team Member 2 -Shubham Kodape*\n",
        "##### *Team Member 3 -Asma Patel*\n",
        "##### *Team Member 4 -anuja Ghotekar*\n",
        "##### *Team Member 5 -Pratiksha Auti*\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bike Sharing Demand Prediction project was done by group of 5 members -Tabassum patel,Shubham Kodape,Asma patel,Anuja ghotekar,Pratiksha Auti.\n",
        "We started on working on csv data individually.As dataset was quite big enough which led more computation time.we felt little challenging when we start working on different algorithms  choosing quite number of algorithms to work upon .Also deciding about the best model for prediction. \n",
        "Then we decided to divide Regression important topics among 5 and started exploring data set.\n",
        "Pratiksha- Check the description (overall and column) ,check the null value,treat null value,or go with mean,median or mode according to need.\n",
        "Anuja- check and treat outliers.Either remove them (if they are less in number),Use some techniques. \n",
        "Shubham- Go for distribution checking ,treat distribution.It can be right skewed or left.Check model by fitting all data without doing any transformation and check accuracy in both cases.\n",
        "Asma - Check the correlation and treat if it is high.Heatmap or corr()Treat -either merge two columns or drop one imp column.\n",
        "Tabassum- Model creation part.Linear regression,Non linear model.Implement hyper parameter tuning.\n",
        "After all this process we got EDA insights and Result of ML models , All 4 models have been explained with the help of SHAP library."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. https://github.com/Asmapatel21\n",
        "\n",
        "2. https://github.com/Anujaghotekar\n",
        "\n",
        "3. https://github.com/Prati15s\n",
        "\n",
        "4. https://github.com/Shubham999-code\n",
        "\n",
        "5. https://github.com/TabassumPatel4458"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Project Flowchart:\n",
        "#####Initial preparations(Loading the dependencies and the data)\n",
        "\n",
        "EDA\n",
        "\n",
        "Clean-Up\n",
        "\n",
        "Feature engineering\n",
        "\n",
        "Pre processing of the data\n",
        "\n",
        "Model implementation\n",
        "\n",
        "Model explainability"
      ],
      "metadata": {
        "id": "WajHuK95ekKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from scipy.stats import skew\n",
        "%matplotlib inline\n",
        "\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "bike_sharing_df = pd.read_csv('/content/SeoulBikeData.csv',encoding='unicode_escape')\n",
        "bike_sharing_df=pd.read_csv('/content/SeoulBikeData.csv',encoding='unicode_escape')\n",
        "\n",
        "df1 =bike_sharing_df.copy()\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First 5 observations\n",
        "bike_sharing_df.head()"
      ],
      "metadata": {
        "id": "sowqOn9RYZTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Last 5 observations\n",
        "bike_sharing_df.tail()"
      ],
      "metadata": {
        "id": "osLR625GYeVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "bike_sharing_df.shape\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsuaBfwLlXrZ"
      },
      "source": [
        "#Getting all the columns\n",
        "print(\"Features of the dataset:\")\n",
        "df1.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OCJGIQnlnHE"
      },
      "source": [
        "#print the unique value\n",
        "df1.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn1nhf1-lptW"
      },
      "source": [
        "#Looking for the description of the dataset to get insights of the data\n",
        "df1.describe().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tpb5LHPsPehD"
      },
      "source": [
        "* ***This Dataset contains 8760 lines and 14 columns.*** \n",
        "***In a day we have 24 hours and we have 365 days a year so 365 multiplied by 24 = 8760, which represents the number of line in the dataset.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMOeshii9FP6"
      },
      "source": [
        "##Features description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kIJJWuiRZE8"
      },
      "source": [
        "**Breakdown of Our Features:**\n",
        "\n",
        "**Date** : *The date of the day, during 365 days from 01/12/2017 to 30/11/2018, formating in DD/MM/YYYY, type : str*, we need to convert into datetime format.\n",
        "\n",
        "**Rented Bike Count** : *Number of rented bikes per hour which our dependent variable and we need to predict that, type : int*\n",
        "\n",
        "**Hour**: *The hour of the day, starting from 0-23 it's in a digital time format, type : int, we need to convert it into category data type.*\n",
        "\n",
        "**Temperature(°C)**: *Temperature in Celsius, type : Float*\n",
        "\n",
        "**Humidity(%)**: *Humidity in the air in %, type : int*\n",
        "\n",
        "**Wind speed (m/s)** : *Speed of the wind in m/s, type : Float*\n",
        "\n",
        "**Visibility (10m)**: *Visibility in m, type : int*\n",
        "\n",
        "**Dew point temperature(°C)**: *Temperature at the beggining of the day, type : Float*\n",
        "\n",
        "**Solar Radiation (MJ/m2)**: *Sun contribution, type : Float*\n",
        "\n",
        "**Rainfall(mm)**: *Amount of raining in mm, type : Float*\n",
        "\n",
        "**Snowfall (cm)**: *Amount of snowing in cm, type : Float*\n",
        "\n",
        "**Seasons**: *Season of the year, type : str, there are only 4 season's in data *. \n",
        "\n",
        "**Holiday**: *If the day  is holiday period or not, type: str*\n",
        "\n",
        "**Functioning Day**: *If the day is a Functioning Day or not, type : str*\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.\n",
        "####Attribute Information:\n",
        "#####Date : year-month-day\n",
        "#####Rented Bike count - Count of bikes rented at each hour\n",
        "#####Hour - Hour of he day\n",
        "#####Temperature-Temperature in Celsius\n",
        "#####Humidity - %\n",
        "#####Windspeed - m/s\n",
        "#####Visibility - 10m\n",
        "#####Dew point temperature - Celsius\n",
        "#####Solar radiation - MJ/m2\n",
        "#####Rainfall - mm\n",
        "#####Snowfall - cm\n",
        "#####Seasons - Winter, Spring, Summer, Autumn\n",
        "#####Holiday - Holiday/No holiday\n",
        "#####Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df1.columns\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df1.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####1.first we understand the nature of dataset then take a variable called bike_sharing_df and apply a .columns property which returns the label of each column in the DataFrame were it fecthing all the csv data called bike sharing and give output in the form of columns.\n",
        "\n",
        "2.in other dataset we are using same variable known as bike_sharing_df and using .describe() method which returns description of the data in the DataFrame and it fecting all the csv data which is bike sharing so that it give output in descriptive way."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating function to return all the unique values each categorical column can have\n",
        "def cat_unique_vals(cat_cols,df):\n",
        "  for col in cat_cols:\n",
        "    print(\"The values that the categorical column\",col,\"can take are:\",df[col].unique())"
      ],
      "metadata": {
        "id": "k35BEYHXcmSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the possible values important and meaningful categorical columns can have.\n",
        "categorical_columns=['Seasons','Holiday']\n",
        "cat_unique_vals(categorical_columns,bike_sharing_df)"
      ],
      "metadata": {
        "id": "4JiRflndcrwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a function that performs a groupby operation and returns a dataframe for analysis\n",
        "def create_df_analysis(col):\n",
        "  return df1.groupby(col)['Rented Bike Count'].sum().reset_index()"
      ],
      "metadata": {
        "id": "5iS3ZdyJcwAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Some of  the columns name in the dataset are too large and clumsy so we change the the into some simple name, and it don't affect our end results.**"
      ],
      "metadata": {
        "id": "jCj4piqp7Pgr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJsAn2lGrtGE"
      },
      "source": [
        "#Rename the complex columns name\n",
        "df1= df1.rename(columns={'Rented Bike Count':'Rented_Bike_Count',\n",
        "                                'Temperature(°C)':'Temperature',\n",
        "                                'Humidity(%)':'Humidity',\n",
        "                                'Wind speed (m/s)':'Wind_speed',\n",
        "                                'Visibility (10m)':'Visibility',\n",
        "                                'Dew point temperature(°C)':'Dew_point_temperature',\n",
        "                                'Solar Radiation (MJ/m2)':'Solar_Radiation',\n",
        "                                'Rainfall(mm)':'Rainfall',\n",
        "                                'Snowfall (cm)':'Snowfall',\n",
        "                                'Functioning Day':'Functioning_Day'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhBbybIqXpyf"
      },
      "source": [
        "* ***Python read \"Date\" column as a object type basically it reads as a string, as the date column is very important to analyze the users behaviour so we need to convert it into datetime format then we split it into 3 column i.e 'year', 'month', 'day'as a category data type.***\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z5CrRQG970G"
      },
      "source": [
        "##Breaking date column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eufvwXItCKRM"
      },
      "source": [
        "# Changing the \"Date\" column into three \"year\",\"month\",\"day\" column\n",
        "df1['Date'] = df1['Date'].apply(lambda x: \n",
        "                                    dt.datetime.strptime(x,\"%d/%m/%Y\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoHGDnVwE2yS"
      },
      "source": [
        "df1['year'] = df1['Date'].dt.year\n",
        "df1['month'] = df1['Date'].dt.month\n",
        "df1['day'] = df1['Date'].dt.day_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKlJesB8Lz6U"
      },
      "source": [
        "#creating a new column of \"weekdays_weekend\" and drop the column \"Date\",\"day\",\"year\"\n",
        "df1['weekdays_weekend']=df1['day'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )\n",
        "df1=df1.drop(columns=['Date','day','year'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEpKbW1Jgly"
      },
      "source": [
        "* ***So we convert the \"date\" column into 3 different column i.e \"year\",\"month\",\"day\".***\n",
        "* ***The \"year\" column in our data set is basically contain the 2 unique number contains the details of from 2017 december to 2018 november so if i consider this is a one year then we don't need the \"year\" column so we drop it***.\n",
        "* ***The other column \"day\", it contains the details about the each day of the month, for our relevence we don't need each day of each month data but we need the data about, if a day is a weekday or a weekend so we convert it into this format and drop the \"day\" column***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puejV1lZtidR"
      },
      "source": [
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtZ3X0aOPot6"
      },
      "source": [
        "df1.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5-dVDBbRh5C"
      },
      "source": [
        "df1['weekdays_weekend'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjr3ul2s-aD0"
      },
      "source": [
        "##Changing data type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ5oXNTTIcnb"
      },
      "source": [
        "* ***As \"Hour\",\"month\",\"weekdays_weekend\" column are show as a integer data type but actually it is a category data tyepe. so we need to change this data tyepe if we not then, while doing the further anlysis and correleted with this then the values are not actually true so we can mislead by this.***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.nunique()"
      ],
      "metadata": {
        "id": "DFzJlvmP80_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAe8T6hvus9h"
      },
      "source": [
        "#Change the int64 column into catagory column\n",
        "cols=['Hour','month','weekdays_weekend']\n",
        "for col in cols:\n",
        "  df1[col]=df1[col].astype('category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dfI7GjvUhR8"
      },
      "source": [
        "#let's check the result of data type\n",
        "df1.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi3-37YR2K26"
      },
      "source": [
        "df1.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['weekdays_weekend'].unique()"
      ],
      "metadata": {
        "id": "7-NJOLqYBNO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOc9bQWSwMQb"
      },
      "source": [
        "# **Exploratory Data Analysis Of The Data Set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f70KNT6OaEfY"
      },
      "source": [
        "**Why do we perform EDA?**\n",
        "* ***An EDA is a thorough examination meant to uncover the underlying structure of a data set and is important for a company because it exposes trends, patterns, and relationships that are not readily apparent.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paU12Mpue04N"
      },
      "source": [
        "## **Univariate Analysis**\n",
        "**Why do you do univariate analysis?**\n",
        "\n",
        " The key objective of Univariate analysis is to simply describe the data to find patterns within the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipcRq2wiYNDZ"
      },
      "source": [
        "## **Analysis of Dependent Variable:**\n",
        "**What is a dependent variable in data analysis?**\n",
        " \n",
        " we analyse our dependent variable,A dependent variable is a variable whose value will change depending on the value of another variable.***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU4LvLye5iU2"
      },
      "source": [
        "##**Analysation of categorical variables**\n",
        " Our dependent variable is \"Rented Bike Count\" so we need to analysis this column with the other columns by using some visualisation plot.\n",
        " first we analyze the category data tyep then we proceed with the numerical data type."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FqqcaAkl1B7"
      },
      "source": [
        "####Season"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a function that performs a groupby operation and returns a dataframe for analysis\n",
        "def create_df_analysis(col):\n",
        "  return bike_sharing_df.groupby(col)['Rented Bike Count'].sum().reset_index()"
      ],
      "metadata": {
        "id": "ZKdHlhYcLzxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Seasons column\n",
        "seasons_col = create_df_analysis('Seasons')\n",
        "seasons_col"
      ],
      "metadata": {
        "id": "bMlVELKbc1fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a visualisation for the seasons column\n",
        "plt.figure(figsize=(10,7))\n",
        "splot = sns.barplot(data=seasons_col,x='Seasons',y='Rented Bike Count')\n",
        "for p in splot.patches:\n",
        "    splot.annotate(format(p.get_height(), '.1f'), \n",
        "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
        "                   ha = 'center', va = 'center', \n",
        "                   xytext = (0, 9), \n",
        "                   textcoords = 'offset points')\n",
        "plt.xlabel(\"Seasons\", size=14)\n",
        "plt.ylabel(\"Rented Bike Count\", size=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dn1SAY1tc3Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial preparations for plotting pie chart with percentages\n",
        "seasons_list = list(seasons_col['Seasons']) \n",
        "rented_count_list = list(seasons_col['Rented Bike Count'])\n",
        "palette_color = sns.color_palette('bright')\n",
        "explode = (0.05,0.05,0.05,0.05)"
      ],
      "metadata": {
        "id": "7JFzrNXDc_3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the pie chart visualisation for seasons column\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.pie(rented_count_list,labels=seasons_list,colors=palette_color,explode=explode,autopct='%0.0f%%')\n",
        "plt.title(\"Percentage of total number of bikes rented for each season\")\n",
        "plt.axis(\"equal\")  \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MTMK5t64dBr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can see that summer has the highest number of bikes rented, 37%. This could be because of the vacation mood created in summer and also the increase in the number of tourists. Winter however is the season where the least number of bikes are rented, 8%.**"
      ],
      "metadata": {
        "id": "sa9WILpwdJR9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0rebrR66k0H"
      },
      "source": [
        "####Month"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(20,8))\n",
        "sns.barplot(data=df1,x='month',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to month ')"
      ],
      "metadata": {
        "id": "Qsif0yhcl7Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjOCFBJbL5Jw"
      },
      "source": [
        "* ***From the above bar plot we can clearly say that from  the month 5 to 10 the demand of the rented bike is high as compare to other months.these months are comes inside the summer season.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya-zVnB_6qDR"
      },
      "source": [
        "####weekdays_weekend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq4RCZKYjDfG"
      },
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(10,8))\n",
        "sns.barplot(data=df1,x='weekdays_weekend',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to weekdays and weekend ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEcLbDEfoEve"
      },
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(20,8))\n",
        "sns.pointplot(data=df1,x='Hour',y='Rented_Bike_Count',hue='weekdays_weekend',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to weekdays_weekend ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-_BpwXlNozJ"
      },
      "source": [
        "* ***From the above point plot and bar plot we can say that in the week days which represent in blue colur show that the demand of the bike higher because of the office.***\n",
        "***Peak Time are 7 am to 9 am and 5 pm to 7 pm***\n",
        "***The orange colur represent the weekend days, and it show that the demand of rented bikes are very low specially in the morning hour but when the evening start from 4 pm to 8 pm the demand slightly increases.***   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SrGKfx36y13"
      },
      "source": [
        "####Hour"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QZMJ9Uc2lKH"
      },
      "source": [
        "#anlysis of data by vizualisation\n",
        "hour_df = create_df_analysis(\"Hour\")\n",
        "hour_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ndfw64yPY8x"
      },
      "source": [
        "* ***In the above plot which shows the use of rented bike according the hours and the data are from all over the year.***\n",
        "\n",
        "* ***generally people use rented bikes during their working hour from 7am to 9am and 5pm to 7pm.***   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RofVq5fJ6De7"
      },
      "source": [
        "####Holiday"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2_JJ7832erx"
      },
      "source": [
        "#anlysis of data by vizualisation\n",
        "holidays_col = create_df_analysis('Holiday')\n",
        "holidays_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eUeqH1VzHj9"
      },
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(20,8))\n",
        "sns.pointplot(data=df1,x='Hour',y='Rented_Bike_Count',hue='Holiday',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to Holiday ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWgfYNEVSAPE"
      },
      "source": [
        "* ***In the above bar plot and point plot which shows the use of rented bike in a holiday, and it clearly shows that,***\n",
        "* ***plot shows that in holiday people uses the rented bike from 2pm-8pm***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA3wpDxr6a-9"
      },
      "source": [
        "####Functioning Day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETqGOa3h3VgV"
      },
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(10,8))\n",
        "sns.barplot(data=df1,x='Functioning_Day',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Functioning Day ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7GS-G9yzT5N"
      },
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(20,8))\n",
        "sns.pointplot(data=df1,x='Hour',y='Rented_Bike_Count',hue='Functioning_Day',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to Functioning Day ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCDdEgm6Q0K5"
      },
      "source": [
        "* ***In the above bar plot and point plot which shows the use of rented bike in functioning daya or not, and it clearly shows that,***\n",
        "* ***Peoples dont use reneted bikes in no functioning day.*** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzC5zzAW7KFz"
      },
      "source": [
        "##**Analyze of Numerical variables**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6m2bjNvx-UB"
      },
      "source": [
        "**What is Numerical Data**\n",
        "\n",
        "* ***Numerical data is a data type expressed in numbers, rather than natural language description. Sometimes called quantitative data, numerical data is always collected in number form. Numerical data differentiates itself from other number form data types with its ability to carry out arithmetic operations with these numbers.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbv7jj3J5Ina"
      },
      "source": [
        "#### Analyze of Numerical variables distplots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQbFN4Exs4Rp"
      },
      "source": [
        "#assign the numerical coulmn to variavle\n",
        "numerical_columns=list(df1.select_dtypes(['int64','float64']).columns)\n",
        "numerical_features=pd.Index(numerical_columns)\n",
        "numerical_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M090PPQK5xoX"
      },
      "source": [
        "#printing displots to analyze the distribution of all numerical features\n",
        "for col in numerical_features:\n",
        "  plt.figure(figsize=(10,6))\n",
        "  sns.distplot(x=df1[col])\n",
        "  plt.xlabel(col)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46qJ6Wl54_pX"
      },
      "source": [
        "#### Numerical vs.Rented Bike Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APjfF6zhzssN"
      },
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Temperature\" \n",
        "df1.groupby('Temperature').mean()['Rented_Bike_Count'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-SJK0ppWdsQ"
      },
      "source": [
        "* ***From the above plot we see that people like to ride bikes when it is pretty hot around 25°C in average***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK5ZlSy6XKm3"
      },
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Dew_point_temperature\" \n",
        "df1.groupby('Dew_point_temperature').mean()['Rented_Bike_Count'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7jbHSBmoLVC"
      },
      "source": [
        "* ***From the above plot of \"Dew_point_temperature' is almost same as the 'temperature' there is some similarity present we can check it in our next step.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7WX-ed40N_s"
      },
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Solar_Radiation\" \n",
        "df1.groupby('Solar_Radiation').mean()['Rented_Bike_Count'].plot()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jihtj207XAe1"
      },
      "source": [
        "* ***from the above plot we see that, the amount of rented bikes is huge, when there is solar radiation, the counter of rents is around 1000***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N_XtCX_2EA2"
      },
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Snowfall\" \n",
        "df1.groupby('Snowfall').mean()['Rented_Bike_Count'].plot()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m8UJdiaXSXj"
      },
      "source": [
        "* ***We can see from the plot that, on the y-axis, the amount of rented bike is very low When we have more than 4 cm of snow, the bike rents is much lower***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vg3QvKG2NNS"
      },
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Rainfall\" \n",
        "df1.groupby('Rainfall').mean()['Rented_Bike_Count'].plot()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlMz0lCkXmLu"
      },
      "source": [
        "* ***We can see from the above plot that even if it rains a lot the demand of of rent bikes is not decreasing, here for example even if we have 20 mm of rain there is a big peak of rented bikes***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtVoNDubp6tn"
      },
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Wind_speed\" \n",
        "df1.groupby('Wind_speed').mean()['Rented_Bike_Count'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrNH9L4erDbA"
      },
      "source": [
        "* ***We can see from the above plot that the demand of rented bike is uniformly distribute despite of wind speed but when the speed of wind was 7 m/s then the demand of bike also increase that clearly means peoples love to ride bikes when its little windy.***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a dataframe for analysing the number of bikes rented for different humidity percentages. \n",
        "humidity_bike = create_df_analysis('Humidity(%)')\n",
        "humidity_bike"
      ],
      "metadata": {
        "id": "VH6EN0zCz2SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting a visualisation for the different humidity percentages\n",
        "plt.plot(humidity_bike['Humidity(%)'],humidity_bike['Rented Bike Count'])\n",
        "plt.xlabel('Humidity(%)')\n",
        "plt.ylabel(\"Rented Bike Count\")\n",
        "plt.title(\"Number of bikes rented across different humidity percentages\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FxpHTd9wz6M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can see that the majority of the bikes are rented for a humidity percentage range of 30 to 70.**"
      ],
      "metadata": {
        "id": "d4C7Z1gY0AEM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqhG9ri94nrk"
      },
      "source": [
        "####Regression plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHjxYv4hxqpC"
      },
      "source": [
        "* ***The regression plots in seaborn are primarily intended to add a visual guide that helps to emphasize patterns in a dataset during exploratory data analyses. Regression plots as the name suggests creates a regression line between 2 parameters and helps to visualize their linear relationships.***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the regression plot for all the numerical features\n",
        "for col in numerical_features:\n",
        "  fig,ax=plt.subplots(figsize=(10,6))\n",
        "  sns.regplot(x=df1[col],y=df1['Rented_Bike_Count'],scatter_kws={\"color\": 'blue'}, line_kws={\"color\": \"black\"})\n",
        "  corr=df1[col].corr(df1['Rented_Bike_Count'])\n",
        "  ax.set_title('Column'+ col+'vs Rented Bike Count - correlation:'+str(corr))"
      ],
      "metadata": {
        "id": "A092vSGDtW-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Engineering & Data Pre-processing**"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdJN0EU7VoCE"
      },
      "source": [
        "**Why do we need to handle missing values?**\n",
        "* ***The real-world data often has a lot of missing values. The cause of missing values can be data corruption or failure to record data. The handling of missing data is very important during the preprocessing of the dataset as many machine learning algorithms do not support missing values.that's why we check missing values first*** "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df1.drop_duplicates(inplace = True)\n",
        "df1.isna().sum()\n",
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1oCaCrzV8mN"
      },
      "source": [
        "* ***As we can see above there are no missing value presents thankfully***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGuIHav-9b0Y"
      },
      "source": [
        "##Duplicate values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5FryvjqWwFH"
      },
      "source": [
        "**Why is it important to remove duplicate records from my data?** \n",
        "\n",
        "\"Duplication\" just means that you have repeated data in your dataset. This could be due to things like data entry errors or data collection methods. by removing duplication in our data set,  Time and money are saved by not sending identical communications multiple times to the same person."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gQDrqCUq_Qx"
      },
      "source": [
        "# Checking Duplicate Values\n",
        "value=len(df1[df1.duplicated()])\n",
        "print(\"The number of duplicate values in the data set is = \",value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INta7FkIUtje"
      },
      "source": [
        "* ***In the above data after count the missing and duplicate value we came to know that there are no missing and duplicate value present.***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers are those data points that are significantly different from the rest of the data points in the dataset. These can cause the data to skew and affect the accuracy of the ML model."
      ],
      "metadata": {
        "id": "r7hXTw7o3Uyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a list of columns that can possibly contain outliers\n",
        "possible_outlier_cols = list(set(bike_sharing_df.describe().columns)-{'Rented Bike Count','Hour'})\n",
        "possible_outlier_cols"
      ],
      "metadata": {
        "id": "tGY_ofDw3WEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a boxplot to detect columns with outliers\n",
        "plt.figure(figsize=(10,7))\n",
        "for index,item in enumerate(possible_outlier_cols):\n",
        "  plt.subplot(2,4,index+1)\n",
        "  sns.boxplot(bike_sharing_df[item])"
      ],
      "metadata": {
        "id": "yb9PXth63cdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we can see that the columns that contain outliers are Rainfall, Snowfall, Windspeed and Solar Radiation**"
      ],
      "metadata": {
        "id": "WN95YsQ73j2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a list of columns that contains outliers\n",
        "outlier_cols = ['Rainfall(mm)','Wind speed (m/s)','Snowfall (cm)','Solar Radiation (MJ/m2)']\n",
        "outlier_cols"
      ],
      "metadata": {
        "id": "2MJB0j6p3qMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the inter-quartile range for the columns with outliers\n",
        "Q1 = bike_sharing_df[outlier_cols].quantile(0.25)\n",
        "Q3 = bike_sharing_df[outlier_cols].quantile(0.75)\n",
        "IQR = Q3-Q1\n",
        "IQR"
      ],
      "metadata": {
        "id": "d88jD-iq3ttV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the upper and lower fence for outlier removal\n",
        "u_fence = Q3 + (1.5*IQR)\n",
        "l_fence = Q1 - (1.5*IQR)"
      ],
      "metadata": {
        "id": "2HY4nrVI3w6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Detecting and removing the outliers\n",
        "bike_sharing_df[outlier_cols] = bike_sharing_df[outlier_cols][~((bike_sharing_df[outlier_cols] < l_fence) | (bike_sharing_df[outlier_cols] > u_fence))]"
      ],
      "metadata": {
        "id": "iBBBAk8D3xlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the number of outliers deleted\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "wX9CGqeh344w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Due to outlier deletion, some null values have been created in these 4 columns.Now, we can either delete the observations with null values or impute them with some meaning full values. In this case I will be imputing them with the median value of each column.**"
      ],
      "metadata": {
        "id": "7J60mY3r4WxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a function to impute the null values created by outlier handling.\n",
        "def impute_null(outlier_cols):\n",
        "  for col in outlier_cols:\n",
        "    bike_sharing_df[col].fillna(bike_sharing_df[col].median(),inplace=True)"
      ],
      "metadata": {
        "id": "0YlVxdGz4dNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling the imputing function\n",
        "impute_null(outlier_cols)"
      ],
      "metadata": {
        "id": "Nf0eVaM_4_ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if the imputation is successful\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "JyProj2p5Cag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The imputation is successful and therefore we have handled the outliers successfully**"
      ],
      "metadata": {
        "id": "TfvdQSc55Iiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Feature Engineering:**"
      ],
      "metadata": {
        "id": "iGcJElLa7izX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature encoding:\n",
        "####Machine learning models can only work with numerical values and therefore important categorical columns have to converted/encoded into numerical variables. This process is known as Feature Encoding"
      ],
      "metadata": {
        "id": "jfm01lka7rby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have three columns that require encoding and they are Seasons, Holiday and Functioning Day."
      ],
      "metadata": {
        "id": "stJrVex-75z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding for Seasons column\n",
        "bike_sharing_df['Winter'] = np.where(bike_sharing_df['Seasons']=='Winter', 1, 0)\n",
        "bike_sharing_df['Spring'] = np.where(bike_sharing_df['Seasons']=='Spring', 1, 0)\n",
        "bike_sharing_df['Summer'] = np.where(bike_sharing_df['Seasons']=='Summer', 1, 0)\n",
        "bike_sharing_df['Autumn'] = np.where(bike_sharing_df['Seasons']=='Autumn', 1, 0)\n",
        "\n",
        "#Removing seasons column since we dont require it now.\n",
        "bike_sharing_df.drop(columns=['Seasons'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "5FnKg1zO748I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding for Holiday column\n",
        "bike_sharing_df['Holiday'] = np.where(bike_sharing_df['Holiday']=='Holiday',1,0)"
      ],
      "metadata": {
        "id": "WMZ38OmJ8Xz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding for Functioning day\n",
        "bike_sharing_df['Functioning Day'] = np.where(bike_sharing_df['Functioning Day']=='Yes',1,0)"
      ],
      "metadata": {
        "id": "s3qHW7XH8cXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Look of the dataframe after encoding all three columns\n",
        "df1.head()\n"
      ],
      "metadata": {
        "id": "tqeWHoct8im-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Checking correlation for feature removal:**"
      ],
      "metadata": {
        "id": "HNofYicB8reQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting correlation matrix using sns.heatmap\n",
        "corr_matrix = df1.corr()\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(corr_matrix,annot=True,cmap='coolwarm')\n",
        "plt.title('Correlation between the variables of Bike Sharing df')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wx4ImM6S891u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Dew point temperature column is highly correlated to the Temperature column and therefore we can remove it and it most likely won't affect our model building much, so we will drop the dew point temperature column.**"
      ],
      "metadata": {
        "id": "giQnS1xOuflM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will also remove the date column because I don't think it adds any value."
      ],
      "metadata": {
        "id": "oP3b93jOutoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping unnecessary columns\n",
        "bike_sharing_df.drop(columns={'Dew point temperature(°C)','Date'},axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "c7u-_-2SuxxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Removing Multicollinearity:**"
      ],
      "metadata": {
        "id": "u8Ac6IRNwF-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multicollinearity is when two independent variables are highly correlated to each other.\n",
        "\n",
        "Multicollinearity affects the accuracy of the regression models, and therefore we will check if we have multicollinearity in our dataset and solve it by removing columns which cause multicollinearity."
      ],
      "metadata": {
        "id": "96IR1lghwN6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a list of independent columns\n",
        "idv_cols = list(set(bike_sharing_df.columns)-{'Rented Bike Count'})\n"
      ],
      "metadata": {
        "id": "Zg4XWkCqy0JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a function to calculate the variance inflation factor(VIF)\n",
        "def calc_vif(X):\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"Columns\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "40eD25hGy9jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the VIF for independent columns\n",
        "calc_vif(bike_sharing_df[idv_cols])"
      ],
      "metadata": {
        "id": "sxOa4FYczCn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the seasons encoding have very high VIF therefore we will eliminate one of the columns. We will drop winter because it has the lowest bikes rented. The columns Rainfall and Snowfall have no VIF at all so we will drop them too."
      ],
      "metadata": {
        "id": "9FYxhvP2zIEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping the Unnecessary columns\n",
        "bike_sharing_df.drop(columns={'Winter','Rainfall(mm)','Snowfall (cm)'},axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "4-0L89hgzI-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a list of remaining independent columns \n",
        "idv_cols = list(set(bike_sharing_df.columns)-{'Rented Bike Count'})"
      ],
      "metadata": {
        "id": "tTp-eDlFzP2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating VIF for remaining independent columns\n",
        "calc_vif(bike_sharing_df[idv_cols])"
      ],
      "metadata": {
        "id": "9Bs9uR3NzSgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that Functioning Day has a VIF>10 therefore we will remove it."
      ],
      "metadata": {
        "id": "6YMbqknpzewJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping the Functioning Day column \n",
        "bike_sharing_df.drop(columns={'Functioning Day'},axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "pQXyF5D5zfhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a list with remaining independent columns\n",
        "idv_cols = list(set(bike_sharing_df.columns)-{'Rented Bike Count'})"
      ],
      "metadata": {
        "id": "HWPXWwMqzldf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the VIF for remaining independent columns\n",
        "calc_vif(bike_sharing_df[idv_cols])"
      ],
      "metadata": {
        "id": "v2BZ-Rd2zr7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we don't have any variable with VIF>10 we can move ahead."
      ],
      "metadata": {
        "id": "CZEGxSnszzY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Obtaining correlation between independent and dependent variables:**"
      ],
      "metadata": {
        "id": "Z1UMA-Qlz5Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a regression plot to find this correlation. This also finds if the independent variable has a linear relationship with the dependent variable, which is an assumption that has to be satisfied for models like linear regression."
      ],
      "metadata": {
        "id": "L3uDXZK8ZcsX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7twN9aL53x_I"
      },
      "source": [
        "###Checking in OLS Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVsDrbalz1TC"
      },
      "source": [
        "**Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the module\n",
        "#assign the 'x','y' value\n",
        "import statsmodels.api as sm\n",
        "X = df1[[ 'Temperature','Humidity',\n",
        "       'Wind_speed', 'Visibility','Dew_point_temperature',\n",
        "       'Solar_Radiation', 'Rainfall', 'Snowfall']]\n",
        "Y = df1['Rented_Bike_Count']\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "M9eyFdvLZddj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add a constant column\n",
        "X = sm.add_constant(X)\n",
        "X"
      ],
      "metadata": {
        "id": "WwwS_Ecj84Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## fit a OLS model \n",
        "\n",
        "model= sm.OLS(Y, X).fit()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "updvJxg_89pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26Y0dwM_8UQG"
      },
      "source": [
        "\n",
        "* **R sqauare and Adj Square are near to each other. 40% of variance in the Rented Bike count is  explained by the model.**\n",
        "*  **For F statistic , P value is less than 0.05 for 5% levelof significance.**\n",
        "*  **P value of dew point temp and visibility are very high and they are not significant.**\n",
        "\n",
        "*  **Omnibus tests the skewness and kurtosis of the residuals. Here the value of Omnibus is high., it shows we have skewness in our data.**\n",
        "*  **The condition number is large, 3.11e+04. This might indicate that there are strong multicollinearity or other numerical problems**   \n",
        "*  **Durbin-Watson tests for autocorrelation of the residuals. Here value is less than 0.5. We can say that there exists a positive auto correlation among the variables.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.corr()"
      ],
      "metadata": {
        "id": "0-UNJwes9Ly4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEbvgA96yhed"
      },
      "source": [
        "* ***From the OLS model we find that the 'Temperature' and  'Dew_point_temperature' are highly correlated so we need to drop one of them.***\n",
        "* ***for droping the we check the (P>|t|) value from above table and we can see that the 'Dew_point_temperature' value is higher so we need to drop Dew_point_temperature column***\n",
        "* ***For clarity, we use visualisation i.e heatmap in next step***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45V50sG83jo4"
      },
      "source": [
        "###**Heatmap**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU-8vaxi4do2"
      },
      "source": [
        "* **we check correletion betweeen variables using Correlation heatmap, it is graphical representation of correlation matrix representing correlation between different variables**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## plot the Correlation matrix\n",
        "plt.figure(figsize=(20,8))\n",
        "correlation=df1.corr()\n",
        "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
        "sns.heatmap((correlation),mask=mask, annot=True,cmap='coolwarm')"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo5ujsgMeIvh"
      },
      "source": [
        "***We can observe on the heatmap that on the target variable line the most positively correlated variables to the rent are :***\n",
        "\n",
        "* the temperature\n",
        "* the dew point temperature\n",
        "* the solar radiation\n",
        "\n",
        "***And most negatively correlated variables are:***\n",
        "* Humidity\n",
        "* Rainfall\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLzCUzH1cqgr"
      },
      "source": [
        "* ***From the above correlation heatmap, We see that there is a positive \n",
        "correlation between columns 'Temperature' and 'Dew point temperature' i.e 0.91 so even if we drop this column then it dont affects the outcome of our analysis. And they have the same variations.. so we can drop the column 'Dew point temperature(°C)'.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUAHVSG9gY1Z"
      },
      "source": [
        "#drop the Dew point temperature column\n",
        "df1=df1.drop(['Dew_point_temperature'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "id": "wQ81HjcFB-LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pb_Gz-gYJEB"
      },
      "source": [
        "## Create the dummy variables "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiw8DoHV4zSU"
      },
      "source": [
        "**A dataset may contain various type of values, sometimes it consists of categorical values. So, in-order to use those categorical value for programming efficiently we create dummy variables.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak7-comed6JE"
      },
      "source": [
        "#Assign all catagoriacla features to a variable\n",
        "categorical_features=list(df1.select_dtypes(['object','category']).columns)\n",
        "categorical_features=pd.Index(categorical_features)\n",
        "categorical_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rafBX_vvWAZq"
      },
      "source": [
        "###one hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aWyQTKy4WGV"
      },
      "source": [
        "**A one hot encoding allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGhxEDaKWCwW"
      },
      "source": [
        "#creat a copy\n",
        "bike_df_copy = df1\n",
        "\n",
        "def one_hot_encoding(data, column):\n",
        "    data = pd.concat([data, pd.get_dummies(data[column], prefix=column, drop_first=True)], axis=1)\n",
        "    data = data.drop([column], axis=1)\n",
        "    return data\n",
        "\n",
        "for col in categorical_features:\n",
        "    bike_df_copy = one_hot_encoding(bike_df_copy, col)\n",
        "bike_df_copy.head()       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzjnpKCieZXB"
      },
      "source": [
        "#**Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L5GJWpoBkNg"
      },
      "source": [
        "##**Train Test split for regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6Pfy9UTKSAq"
      },
      "source": [
        "Before, fitting any model it is a rule of thumb to split the dataset into a training and test set. This means some proportions of the data will go into training the model and some portion will be used to evaluate how our model is performing on any unseen data. The proportions may vary from 60:40, 70:30, 75:25 depending on the person but mostly used is 80:20 for training and testing respectively. In this step we will split our data into training and testing set using scikit learn library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mmuDlBQY1Nf"
      },
      "source": [
        "#Assign the value in X and Y\n",
        "X = bike_df_copy.drop(columns=['Rented_Bike_Count'], axis=1)\n",
        "y = np.sqrt(bike_df_copy['Rented_Bike_Count'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-xsqAhsMvmz"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First look of the dependent variable dataset\n",
        "Y.head()"
      ],
      "metadata": {
        "id": "_066VFilbbM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creat test and train data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "zRW5VqWFD7OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df_copy.describe().columns"
      ],
      "metadata": {
        "id": "xye1W6P_D_x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed_qRVo0SLsm"
      },
      "source": [
        "* The mean squared error (MSE) tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them.\n",
        "It’s called the mean squared error as you’re finding the average of a set of errors. The lower the MSE, the better the forecast.\n",
        "\n",
        "* MSE formula = (1/n) * Σ(actual – forecast)2\n",
        "Where:\n",
        "\n",
        "*   n = number of items,\n",
        "* Σ = summation notation,\n",
        "* Actual = original or observed y-value,\n",
        "* Forecast = y-value from regression.\n",
        "\n",
        "* Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors).\n",
        "\n",
        "* Mean Absolute Error (MAE) are metrics used to evaluate a Regression Model. ... Here, errors are the differences between the predicted values (values predicted by our regression model) and the actual values of a variable.\n",
        "\n",
        "* R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n",
        "\n",
        "* Formula for R-Squared\n",
        "\\begin{aligned} &\\text{R}^2 = 1 - \\frac{ \\text{Unexplained Variation} }{ \\text{Total Variation} } \\\\ \\end{aligned} \n",
        "​\n",
        "  \n",
        "* R \n",
        "2\n",
        " =1− \n",
        "Total Variation\n",
        "Unexplained Variation\n",
        "​\n",
        "\n",
        "* Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.\n",
        "​\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Target feature conditioning:**\n"
      ],
      "metadata": {
        "id": "hEwQuoltbg3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of the target feature is observed and in this case because it is a positively skewed distribution it is normalised using square root transformation."
      ],
      "metadata": {
        "id": "csCkwiXibqEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for the distribution of the Target variable\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.title(\"Distribution of the target variable: Rented Bike Count. Skewness=\"+str(skew(bike_sharing_df['Rented Bike Count'])))\n",
        "sns.histplot(data=bike_sharing_df,x='Rented Bike Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bUaCavJQbkBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying square root transformation on the dependent variable\n",
        "Y = np.sqrt(Y)"
      ],
      "metadata": {
        "id": "UmY7_z4XbuOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a dataframe with values of Y for visualisation purposes\n",
        "vis_Y = Y.reset_index()"
      ],
      "metadata": {
        "id": "hvsLg3YObxBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking how well the square root transformation has worked\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.title(\"Distribution of the target variable: Rented_Bike_Count. Skewness=\"+str(skew(vis_Y['Rented_Bike_Count'])))\n",
        "sns.histplot(data=vis_Y,x='Rented_Bike_Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4qEiyN8dbzm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can see that the target variable has been normalised and we are good to go.**"
      ],
      "metadata": {
        "id": "UGDqp7YWb2TL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating the test and train dataset:**"
      ],
      "metadata": {
        "id": "8wB50qZub76j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the dataset into test and train datasets\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25,random_state=0)"
      ],
      "metadata": {
        "id": "h92rzT9scIBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shape of the train set of the independent values\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "Mxb75b18cKGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shape of the test set of the independent values\n",
        "X_test.shape"
      ],
      "metadata": {
        "id": "0ywR9qyccOdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Scaling:**"
      ],
      "metadata": {
        "id": "5iVYVzYqcUnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values."
      ],
      "metadata": {
        "id": "VfDE3GonciSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two ways of feature scaling:\n",
        "\n",
        "Min max normalization\n",
        "\n",
        "Standardisation"
      ],
      "metadata": {
        "id": "-mSZAK8HclZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project I'm going to use the standardisation method with the help of the StandardScaler() function."
      ],
      "metadata": {
        "id": "ois3Cvu8cmlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating object for the StandardScaler function\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "qtLXIJQBctAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardizing the independent variables\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "1MBikx6lcwlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Overview of what a dataframe looks like after standardizing\n",
        "X_train"
      ],
      "metadata": {
        "id": "6x4E_tG2d77u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Implementation:**"
      ],
      "metadata": {
        "id": "fwqcfLOAeFag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning models can be described as programs that are trained to find patterns or trends within data and predict the result for new data.\n",
        "\n",
        "In this project we are dealing with a regression problem, therefore we will be using regression models. Some popular examples are Linear Regression and polynomial regression.\n",
        "\n",
        "In this project we will be include the following models:\n",
        "\n",
        "1.Linear regression.\n",
        "\n",
        "2.Ridge regression\n",
        " (Linear regression with L2 regularization).\n",
        "\n",
        "3.Lasso regression (Linear regression with L1 regularization).\n",
        "\n",
        "4.Random forest regression."
      ],
      "metadata": {
        "id": "ZUuDU34_eMMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear regression:**"
      ],
      "metadata": {
        "id": "AhPsEdc7eXZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As stated earlier linear regression is a regression technique, and it comes under supervised learning. Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable value (x). So, this regression technique finds out a linear relationship between x (input) and y(output)"
      ],
      "metadata": {
        "id": "1wSVO7TWefsx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuFpl5EMX0ed"
      },
      "source": [
        "Linear regression uses a linear approach to model the relationship between independent and dependent variables. In simple words its a best fit line drawn over the values of independent variables and dependent variable. In case of single variable, the formula is same as straight line equation having an intercept and slope.\n",
        "\n",
        "$$ \\text{y_pred} = \\beta_0 + \\beta_1x$$\n",
        "\n",
        "where $$\\beta_0 \\text{ and } \\beta_1$$ are intercept and slope respectively.\n",
        "\n",
        "In case of multiple features the formula translates into:\n",
        "\n",
        "$$ \\text{y_pred} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 +\\beta_3x_3 +.....$$\n",
        "\n",
        "where x_1,x_2,x_3 are the features values and \n",
        "$$\\beta_0,\\beta_1,\\beta_2.....$$\n",
        " are weights assigned to each of the features. These become the parameters which the algorithm tries to learn using Gradient descent.\n",
        "\n",
        "Gradient descent is the process by which the algorithm tries to update the parameters using  a loss function . Loss function is nothing but the diffence between the actual values and predicted values(aka error or residuals). There are different types of loss function but this is the simplest one. Loss function summed over all observation gives the cost functions. The role of gradient descent is to update the parameters till the cost function is minimized i.e., a global minima is reached. It uses a hyperparameter 'alpha' that gives a weightage to the cost function and decides on how big the steps to take. Alpha is called as the learning rate. It is always necesarry to keep an optimal value of alpha as high and low values of alpha might make the gradient descent overshoot or get stuck at a local minima. There are also some basic assumptions that must be fulfilled before implementing this algorithm. They are:\n",
        "\n",
        "1. No multicollinearity in the dataset.\n",
        "\n",
        "2. Independent variables should show linear relationship with dv.\n",
        "\n",
        "3. Residual mean should be 0 or close to 0.\n",
        "\n",
        "4. There should be no heteroscedasticity i.e., variance should be constant along the line of best fit.\n",
        "\n",
        "\n",
        "\n",
        "Let us now implement our first model.\n",
        "We will be using LinearRegression from scikit library.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "D_BaSovhEWI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "reg.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "aYV7MTRQEbsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the coefficeint\n",
        "reg.coef_\n"
      ],
      "metadata": {
        "id": "HXi5vprpEeqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train=reg.predict(X_train)\n",
        "y_pred_test=reg.predict(X_test)"
      ],
      "metadata": {
        "id": "-vje6x5qEhhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error((y_train), (y_pred_train))\n",
        "print(\"MSE :\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_train, y_pred_train)\n",
        "print(\"MAE :\",MAE_lr)\n",
        "\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score(y_train, y_pred_train)\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "_P8oTYVwEtTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COThm_2ro9Kk"
      },
      "source": [
        "**Looks like our r2 score value is 0.60 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "training_df=pd.DataFrame(dict1,index=[1])"
      ],
      "metadata": {
        "id": "qmkgJ9-eFjzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyw8IRA6vXD5"
      },
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error(y_test, y_pred_test)\n",
        "print(\"MSE :\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_test, y_pred_test)\n",
        "print(\"MAE :\",MAE_lr)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score((y_test), (y_pred_test))\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score((y_test), (y_pred_test)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",Adjusted_R2_lr )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q439Br-s_li"
      },
      "source": [
        "**The r2_score for the test set is 0.56. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lGDuN-TjuMJ"
      },
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "test_df=pd.DataFrame(dict2,index=[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pP9VWTiAFwO"
      },
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test),(y_test)-(y_pred_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(y_pred_test)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I6tIwbEmF60D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLT3TO7hFQWB"
      },
      "source": [
        "#**LASSO REGRESSION** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH_MuurYZd7I"
      },
      "source": [
        "# Create an instance of Lasso Regression implementation\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso= Lasso(alpha=1.0, max_iter=3000)\n",
        "# Fit the Lasso model\n",
        "lasso.fit(X_train, y_train)\n",
        "# Create the model score\n",
        "print(lasso.score(X_test, y_test), lasso.score(X_train, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_fUfbaI070I"
      },
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_lasso=lasso.predict(X_train)\n",
        "y_pred_test_lasso=lasso.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaiZtIfwf2a5"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_l= mean_squared_error((y_train), (y_pred_train_lasso))\n",
        "print(\"MSE :\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_train, y_pred_train_lasso)\n",
        "print(\"MAE :\",MAE_l)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score(y_train, y_pred_train_lasso)\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxx68FthpSke"
      },
      "source": [
        "**Looks like our r2 score value is 0.40 that means our model is not able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGhZcnrnE4O1"
      },
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Lasso regression ',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((r2_l),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_l ),2)\n",
        "       }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8UJ3usmf2bQ"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_l= mean_squared_error(y_test, y_pred_test_lasso)\n",
        "print(\"MSE :\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_test, y_pred_test_lasso)\n",
        "print(\"MAE :\",MAE_l)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score((y_test), (y_pred_test_lasso))\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l=(1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zXjNyYipqkO"
      },
      "source": [
        "**The r2_score for the test set is 0.38. This means our linear model is  not performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2hn76K6FO9O"
      },
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Lasso regression ',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((r2_l),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_l ),2),\n",
        "       }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96lAcltS1MDg"
      },
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(np.array(y_pred_test_lasso))\n",
        "plt.plot(np.array((y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWmG8IK-Gna8"
      },
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_lasso),(y_test-y_pred_test_lasso))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTaUdoq_3yiH"
      },
      "source": [
        "# **RIDGE REGRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQPE5K2638M4"
      },
      "source": [
        "#import the packages\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge= Ridge(alpha=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY5iHM2h4QCH"
      },
      "source": [
        "#FIT THE MODEL\n",
        "ridge.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XZWOZEY4JFo"
      },
      "source": [
        "#check the score\n",
        "ridge.score(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb_HOTbfgv_c"
      },
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_ridge=ridge.predict(X_train)\n",
        "y_pred_test_ridge=ridge.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffJPAQp1gv_x"
      },
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_r= mean_squared_error((y_train), (y_pred_train_ridge))\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_train, y_pred_train_ridge)\n",
        "print(\"MAE :\",MAE_r)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score(y_train, y_pred_train_ridge)\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wf9N7kHpZp2"
      },
      "source": [
        "**Looks like our r2 score value is 0.60 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXLnGPFJE7Hs"
      },
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PWM1MjZgv_y"
      },
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_r= mean_squared_error(y_test, y_pred_test_ridge)\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_test, y_pred_test_ridge)\n",
        "print(\"MAE :\",MAE_r)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score((y_test), (y_pred_test_ridge))\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZue8mVhpsIb"
      },
      "source": [
        "**The r2_score for the test set is 0.56. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW2XK6s3FSnm"
      },
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHg_FBZ45fnI"
      },
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot((y_pred_test_ridge))\n",
        "plt.plot((np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcuzpMu5HS4W"
      },
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_ridge),(y_test)-(y_pred_test_ridge))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0u5B6IDEtqd"
      },
      "source": [
        "# **ELASTIC NET REGRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDAN-eVyE2AF"
      },
      "source": [
        "#import the packages\n",
        "from sklearn.linear_model import ElasticNet\n",
        "#a * L1 + b * L2\n",
        "#alpha = a + b and l1_ratio = a / (a + b)\n",
        "elasticnet = ElasticNet(alpha=0.1, l1_ratio=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "348F0uSpE5pt"
      },
      "source": [
        "#FIT THE MODEL\n",
        "elasticnet.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMOo6nmFE7e8"
      },
      "source": [
        "#check the score\n",
        "elasticnet.score(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEbbcZBXhibv"
      },
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_en=elasticnet.predict(X_train)\n",
        "y_pred_test_en=elasticnet.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBvL_Hljhib1"
      },
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_e= mean_squared_error((y_train), (y_pred_train_en))\n",
        "print(\"MSE :\",MSE_e)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_e= mean_absolute_error(y_train, y_pred_train_en)\n",
        "print(\"MAE :\",MAE_e)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_e= r2_score(y_train, y_pred_train_en)\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG9fezdFpcQK"
      },
      "source": [
        "**Looks like our r2 score value is 0.57 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxGggP0xE9BO"
      },
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Elastic net regression ',\n",
        "       'MAE':round((MAE_e),3),\n",
        "       'MSE':round((MSE_e),3),\n",
        "       'RMSE':round((RMSE_e),3),\n",
        "       'R2_score':round((r2_e),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewtCPtH3hib2"
      },
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_e= mean_squared_error(y_test, y_pred_test_en)\n",
        "print(\"MSE :\",MSE_e)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_e= mean_absolute_error(y_test, y_pred_test_en)\n",
        "print(\"MAE :\",MAE_e)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_e= r2_score((y_test), (y_pred_test_en))\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6y00NRKptiz"
      },
      "source": [
        "**The r2_score for the test set is 0.49. This means our linear model is not performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9P1IkCEFVQv"
      },
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Elastic net regression Test',\n",
        "       'MAE':round((MAE_e),3),\n",
        "       'MSE':round((MSE_e),3),\n",
        "       'RMSE':round((RMSE_e),3),\n",
        "       'R2_score':round((r2_e),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By-8mNBaFCM-"
      },
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(np.array(y_pred_test_en))\n",
        "plt.plot((np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JytM5zRmHsOU"
      },
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_en),(y_test)-(y_pred_test_en))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTdFcFDbd3FF"
      },
      "source": [
        "# **RANDOM FOREST**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a Random Forest Regression model using GridSearchCV\n",
        "rand_forest = RandomForestRegressor()\n",
        "\n",
        "\n",
        "parameters = {'n_estimators' : [int(x) for x in np.linspace(start=10,stop=20, num=5)], \n",
        "             'max_depth' : [10,15,20],\n",
        "             'min_samples_split':[2,4],\n",
        "             'min_samples_leaf':[1,2],\n",
        "             'bootstrap' : [True,False]\n",
        "             }\n",
        "\n",
        "rf_model_grid = GridSearchCV(rand_forest,parameters,scoring='r2',cv=5)\n",
        "rf_model_grid.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "jYiViYGBdo9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the best parameters for Random Forest regression fetched through GridSearchCV\n",
        "print(f'The best value for parameters in random forest regression through GridSearchCV is found to be {rf_model_grid.best_params_}')\n",
        "print(f'\\nUsing {rf_model_grid.best_params_} as the value for the parameters in random forest model, it gives us a negative mean squared error of: {rf_model_grid.best_params_}')"
      ],
      "metadata": {
        "id": "DdN9iyDmeBiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best value for parameters in random forest regression through GridSearchCV is found to be {'bootstrap': True, 'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 4, 'n_estimators': 20}\n",
        "\n",
        "Using {'bootstrap': True, 'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 4, 'n_estimators': 20} as the value for the parameters in random forest model, it gives us a negative mean squared error of: 0.7089944921643333"
      ],
      "metadata": {
        "id": "3OCiilxeeEuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting Random Forest model on the dataset with appropriate paramter values\n",
        "rf_model = RandomForestRegressor(bootstrap=True,max_depth=20,min_samples_leaf=2,min_samples_split=4,n_estimators=15).fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "PdeWg0NJeHxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting values of the independent variable on the test set\n",
        "Y_test_pred_rf = rf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "i-ewvNtZeJ9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a function to plot the comparison between actual values and predictions\n",
        "def plot_comparison(y_pred,model):\n",
        "   plt.figure(figsize=(10,7))\n",
        "   plt.title(\"The comparison of actual values and predictions obtained by \"+model) \n",
        "   plt.plot(np.array((Y_test)))\n",
        "   plt.plot((y_pred),color='red')\n",
        "   plt.legend([\"Actual\",\"Predicted\"])\n",
        "   plt.show()"
      ],
      "metadata": {
        "id": "wRpQ2sZagExH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the comparison between actual and predicted values obtained by Random Forest Regression\n",
        "plot_comparison(Y_test_pred_rf,'Random Forest Regression')"
      ],
      "metadata": {
        "id": "GR8-WbLueMPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a function to calculate and display the evaluation metrics for the model\n",
        "def eval_metrics(y_pred,model):\n",
        "  print(\"The evaluation metrics for \"+model+\" are given as:\")\n",
        "  MSE  = mean_squared_error(Y_test,y_pred)      #Mean squared error for test set\n",
        "  print(\"MSE :\" , MSE)\n",
        "  \n",
        "  RMSE = np.sqrt(MSE)\n",
        "  print(\"RMSE :\" ,RMSE)\n",
        "  \n",
        "  r2_test = r2_score(Y_test,y_pred)             #r2 score for prediction on test set\n",
        "  print(\"R2 :\" ,r2_test)\n",
        "  \n",
        "  a_r2_test = 1-(1-r2_score(Y_test,y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))     #adjusted r2 score for test set\n",
        "  print(\"Adjusted R2 :\",a_r2_test)"
      ],
      "metadata": {
        "id": "z4nANrkYgbXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the evaluation metrics for Random Forest Regression\n",
        "eval_metrics(Y_test_pred_rf,'Random Forest Regression')"
      ],
      "metadata": {
        "id": "A4vFgIj2eOoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can see the quality of model prediction has drastically improved.**"
      ],
      "metadata": {
        "id": "YRqzaBBZeRHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Explainability:**"
      ],
      "metadata": {
        "id": "Ql_4IBx1eyTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model explainability refers to the concept of being able to understand the machine learning model. For example – If a healthcare model is predicting whether a patient is suffering from a particular disease or not. The medical practitioners need to know what parameters the model is taking into account or if the model contains any bias. So, it is necessary that once the model is deployed in the real world. Then, the model developers can explain the model.\n",
        "\n",
        "Popular techniques for model explainability:\n",
        "\n",
        "1.LIME\n",
        "\n",
        "2.SHAP\n",
        "\n",
        "3.ELI-5\n",
        "\n",
        "In this project I'll be using SHAP for model explainability. Among the various methods in SHAP I'll be using the SHAP summary plot, which plots features/columns in order of their impact on the predictions and also plots the SHAP values."
      ],
      "metadata": {
        "id": "lnP_qjIPe6Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the shap library\n",
        "!pip install shap"
      ],
      "metadata": {
        "id": "15PH_u5QfFaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialising javascript for visualisation of SHAP\n",
        "import shap"
      ],
      "metadata": {
        "id": "uHckUjE4fH0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a function to plot the shap summary plot\n",
        "def shap_summary(model):\n",
        "   explainer_shap = shap.Explainer(model=model, masker=X_train)\n",
        "   shap_values = explainer_shap.shap_values(X_train)\n",
        "   shap.summary_plot(shap_values,X_train,feature_names=X.columns)"
      ],
      "metadata": {
        "id": "UStzgTy6fJsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting shap summary plot for linear regression\n",
        "shap_summary(reg)"
      ],
      "metadata": {
        "id": "drzjBfZTfMN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting shap summary plot for Ridge regression\n",
        "shap_summary(ridge)"
      ],
      "metadata": {
        "id": "mP_0zsHxfPMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting shap summary plot for Lasso regression\n",
        "shap_summary(lasso)"
      ],
      "metadata": {
        "id": "ru0F-5kOfRVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting shap summary plot for Random forest regression model\n",
        "explainer_shap = shap.Explainer(model=rf_model, masker=X_train)\n",
        "shap_values = explainer_shap.shap_values(X_train,check_additivity=False)\n",
        "shap.summary_plot(shap_values,X_train,feature_names=X.columns)"
      ],
      "metadata": {
        "id": "mRQ9LfDGfT0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that different importance or impact power is given to the features by each model and that defines how well the model performs on prediction. Random forest gives almost all the features a significant impact power and therefore it performs the best out of all the 4 models.\n",
        "\n",
        "By looking at the SHAP summary plot for each model, we can figure out the feature importance and also its impact power by understanding the SHAP values."
      ],
      "metadata": {
        "id": "ukGrDHn2fWIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the time of our analysis, we initially did EDA on all the features of our datset. We first analysed our dependent variable, 'Rented Bike Count' and also transformed it. Next we analysed categorical variable and dropped the variable who had majority of one class, we also analysed numerical variable, found out the correlation, distribution and their relationship with the dependent variable. We also removed some numerical features who had mostly 0 values and hot encoded the categorical variables."
      ],
      "metadata": {
        "id": "96OyOZTvXcc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA insights:\n",
        "\n",
        "* Most number of bikes are rented in the Summer season and the lowest in the winter season.\n",
        "\n",
        "* Over 96% of the bikes are rented on days that are considered as No Holiday.\n",
        "* Most number of bikes are rented in the temperature range of 15 degrees to 30 degrees.\n",
        "\n",
        "* Most number of bikes are rented when there is no snowfall or rainfall.\n",
        "* Majority of the bikes are rented for a humidity percentage range of 30 to 70.\n",
        "* The highest number of bike rentals have been done in the 18th hour, i.e 6pm, and lowest in the 4th hour, i.e 4am.\n",
        "\n",
        "* Most of the bike rentals have been made when there is high visibility."
      ],
      "metadata": {
        "id": "B8KNHWvvW56e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Next we implemented 7 machine learning algorithms Linear Regression,lasso,ridge,elasticnet,decission tree, Random Forest and XGBoost. We did hyperparameter tuning to improve our model performance. The results of our evaluation are:\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying the results of evaluation metric values for all models\n",
        "result=pd.concat([training_df,test_df],keys=['Training set','Test set'])\n",
        "result"
      ],
      "metadata": {
        "id": "XxlcjBUwQwlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVIJju-bG0vc"
      },
      "source": [
        "\n",
        "• No overfitting is seen.\n",
        "\n",
        "• Random forest Regressor and Gradient Boosting gridsearchcv gives the highest R2 score of 99% and 95% recpectively for Train Set and 92% for Test set.\n",
        "\n",
        "• Feature Importance value for Random Forest and Gradient Boost are different.\n",
        "\n",
        "• We can deploy this model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results from ML models:\n",
        "\n",
        "* Random Forest Regression is the best performing model with an r2 score of 0.6645.\n",
        "\n",
        "* Lasso Regression(Lion visualisation is done for all the 4 models.\n",
        "\n",
        "* All 4 models have been explained with the help of SHAP library.\n",
        "\n",
        "* Temperature and Hour are the two most important factors according to all the models."
      ],
      "metadata": {
        "id": "hnI4beolXsRr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oXsTmjB0lJ-"
      },
      "source": [
        "\n",
        "However, this is not the ultimate end. As this data is time dependent, the values for variables like temperature, windspeed, solar radiation etc., will not always be consistent. Therefore, there will be scenarios where the model might not perform well. As Machine learning is an exponentially evolving field, we will have to be prepared for all contingencies and also keep checking our model from time to time. Therefore, having a quality knowledge and keeping pace with the ever evolving ML field would surely help one to stay a step ahead in future."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Challenges faced:\n",
        "\n",
        "* Removing Outliers.\n",
        "\n",
        "* Encoding the categorical columns.\n",
        "\n",
        "* Removing Multicollinearity from the dataset.\n",
        "\n",
        "* Choosing Model explainability technique."
      ],
      "metadata": {
        "id": "YRM7a2m4X47y"
      }
    }
  ]
}